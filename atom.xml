<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-11-10T15:24:44-05:00</updated><id>http://localhost:4000/atom.xml</id><title type="html">In That Number</title><subtitle>A blog about data science practice, ethics, and the things in between.</subtitle><entry><title type="html">From What to How</title><link href="http://localhost:4000/ai-ethics/research-review/review-of-tools/" rel="alternate" type="text/html" title="From What to How" /><published>2020-12-27T19:00:00-05:00</published><updated>2020-12-27T19:00:00-05:00</updated><id>http://localhost:4000/ai-ethics/research-review/review-of-tools</id><content type="html" xml:base="http://localhost:4000/ai-ethics/research-review/review-of-tools/">&lt;h1 id=&quot;from-what-to-how-an-initial-review-of-publicly-available-ai-ethics-tools-methods-and-research-to-translate-principles-into-practices&quot;&gt;From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;A 2019 review of AI ethics tools and methods&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Publisher: Morley, J., Floridi, L., Kinsey, L. et al. From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices. Sci Eng Ethics 26, 2141–2168 (2020). https://doi.org/10.1007/s11948-019-00165-5&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;The authors begin by asserting that code, and particularly code that facilitates machine learning, is both our greatest threat and our greatest promise. Citing a review of AI ethics principles, the argues note that ethically-aligned machine learning is typically defined as having the following properties: beneficence, non-maleficence, autonomy, justice, and explainability. This consensus on the definition of AI creates a foundation upon which technologists can “communicate expectations and evaluate deliverables”. However, very few of these guidelines set technical expectations. The authors note that the next step in defining ethical machine learning guidelines is to translate the &lt;strong&gt;what&lt;/strong&gt; outlined in the principles to &lt;strong&gt;how&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;To bridge this gap, the authors define a typology that maps the foundational principles above to each stage of the machine learning algorithm design process. They identified 425 papers which ostensibly answer the question of “how to develop an ethical algorithmic system”. The typology developed is intended to show machine learning developers what ethical ML tools are already available. The authors find that not all principles have tools for each stage of the algorithm design process. Explainability, which is more easily implemented than the other principles, is overrepresented in the toolset. Unlike beneficence, non-maleficence, autonomy, and justice, explainability is not a moral principle, so tools that enable transparency like LIME and SHAP are insufficient alone. Instead of ensuring explainability at the back end, the authors propose that transparency is prioritized at the beginning of the design process.&lt;/p&gt;

&lt;p&gt;The authors also note that few of the tools provide ways to assess the impact of an ML algorithm on an individual in a dataset. The &lt;em&gt;deployment&lt;/em&gt; column of the typology is blank, highlighting the need for “pro- ethically designed human-computer interaction (at an individual level) or networks of ML systems (at a group level)”. The authors assert that not enough research has been done in the field of translating predictions to decisions in the context of algorithmic systems. Finally, the tools in the typology are generally not usable, as they are either not easily applicable in practice or too complex for a wide set of users, as in the case of open source code libraries.&lt;/p&gt;

&lt;p&gt;ML researchers from all disciplines will need to accept that: (1) AI is built on assumptions; (2) human behaviour is complex; (3) algorithms can have unfair consequences; (4) algorithmic predictions can be hard to interpret (Vaughan &amp;amp; Wallach, 2016); (5) trade-offs are usually inevitable; and (6) positive, ethical features are open to progressive increase. Ethical principles should not be applied once or not, but should be regularly re-applied or applied differently, depending on changing needs throughout the ML development process.&lt;/p&gt;

&lt;h2 id=&quot;my-takeaways&quot;&gt;My Takeaways&lt;/h2&gt;

&lt;p&gt;I don’t think this article provided me with anything I didn’t already know. Through my (albeit limited) review of AI ethics principles, it’s evident that a wide gap exists between framework and technical practice. The guidelines I tend to see fall on one of two extremes: high-level principles that outline the values algorithmic systems should embed, or low-level technical implementations of those values (such as transparency as in SHAP, or fairness as in fairlearn) without explicating a context for prioritizing those results within organizational and/or project goals. I appreciated that the authors’ literature review and typology reflected this sentiment, and that they provided a thorough synthesis of the existing ML ethics conversation.&lt;/p&gt;

&lt;p&gt;I think one of my most important takeaways is at the end of the article, where the authors describe what topics the ML research community prioritize. They put the most important topic at the bottom of the list: &lt;strong&gt;the evaluation and creation of pro-ethical business models and incentive structures that balance the costs and rewards of investing in ethical AI across society&lt;/strong&gt;. I firmly believe that none of the other topics can be on the table, not a commitment to reproducibility and openness, not evaluation of currently implemented tools, not the development of a common language, etc. until ML developers have organizational buy-in. Organizational buy-in only occurs when ethical priorities align with financial priorities, and in some cases the only way to facilitate that alignment may be a fundamental restructuring of financial priorities.&lt;/p&gt;

&lt;p&gt;The authors note that they eventually hope to create a searchable database with AI ethics tools and methods, and I think that would be particularly helpful to practitioners. The authors note an urgent need to progress research in the AI for Social Good field, and having a comprehensive database of tools (with a common/standardized language to discuss concepts) would be a good first step to making that research feasible.&lt;/p&gt;

&lt;h2 id=&quot;notable-quotes&quot;&gt;Notable Quotes&lt;/h2&gt;

&lt;h2 id=&quot;citations&quot;&gt;Citations&lt;/h2&gt;</content><author><name></name></author><category term="ai-ethics" /><category term="research-review" /><summary type="html">From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices</summary></entry><entry><title type="html">Lessons from Archives</title><link href="http://localhost:4000/ai-ethics/research-review/lessons-from-archives/" rel="alternate" type="text/html" title="Lessons from Archives" /><published>2020-12-10T19:00:00-05:00</published><updated>2020-12-10T19:00:00-05:00</updated><id>http://localhost:4000/ai-ethics/research-review/lessons-from-archives</id><content type="html" xml:base="http://localhost:4000/ai-ethics/research-review/lessons-from-archives/">&lt;h1 id=&quot;lessons-from-archives-strategies-for--collecting-sociocultural-data-in-machine-learning&quot;&gt;Lessons from Archives: Strategies for  Collecting Sociocultural Data in Machine Learning&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;What can data scientists learn from the archival sciences about data collection?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Publisher: Conference on Fairness, Accountability, and Transparency (FAT ’20), January 27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3351095.3372829&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Reader, you are likely already aware that haphazardly-collected data can propagate social biases. Most ML research focused on devising state-of-the-art models and benchmarks does not critically examine the data sources used to develop models and benchmarks. Data curation as a whole is not a part of most ML research portfolios. The authors propose an alternate approach to interrogating data informed by archival research best practices. Further, they advocate for the creation of an interdisciplinary field whose mission is “data gathering, sharing, annotation, ethics monitoring, and record-keeping processes.”&lt;/p&gt;

&lt;p&gt;The authors focus on archives because they have many foci that parallel the needs of data curation in ML, including full-time curators, community participation, standardized documentation methods, and professional codes of ethics. Fundamentally, archival science can offer extensive study on the extraction of human information. Archives represent the other extreme of the data collection spectrum, interventionist where ML is unscrupulous. But the authors argue that adapting a more interventionist approach to data collection can help researchers avoid replicating historical and representational biases.&lt;/p&gt;

&lt;p&gt;A list of proposed integrations from archival science that stood out to me include:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Data consortia, which would set up institutional frameworks for ethical data collection to benefit both small and large firms.&lt;/li&gt;
  &lt;li&gt;Resources that ensure transparency, such as Datasheets for Datasets, which record not only the contents of data, but also the process of data collection.&lt;/li&gt;
  &lt;li&gt;A code of ethics for data collection with appropriate incentive measures to ensure compliance.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The authors conclude with a macro-list of action items:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Congregate and develop data consortia&lt;/li&gt;
  &lt;li&gt;Establish professional organizations that work by member-ship to enforce adherence to ethical guidelines&lt;/li&gt;
  &lt;li&gt;Support community archives&lt;/li&gt;
  &lt;li&gt;Develop a subfield dedicated to the data collection and an-notation process&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;my-takeaways&quot;&gt;My Takeaways&lt;/h2&gt;

&lt;p&gt;The idea of treating data collection carefully is not new to me; in economics research, generally an entire section of a paper details data collection methods and the overarching statistical properties of the data. I also have experience with archival research through a research project I completed in college. However, this paper has made me realize that just looking at superficial metadata like summary statistics is woefully inadequate. The process for data curation outlined here is so thorough and thoughtful, and I genuinely want to live in a world where data collection is treated with more reverence like explicated in this research.&lt;/p&gt;

&lt;p&gt;I’m particularly struck by this recommendation to the data science community: “form or integrate existing global/national organizations in instituting standardized codes of ethics/conduct and procedures to review violations.” So often, when I think about ML ethics, it’s hard for me to envision a sufficient incentive structure. I think the authors succeed in depicting the archival model, or something more like it, as a means of safeguarding ML models.&lt;/p&gt;</content><author><name></name></author><category term="ai-ethics" /><category term="research-review" /><summary type="html">Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning</summary></entry><entry><title type="html">Interventions for Algorithmic Equity</title><link href="http://localhost:4000/ai-ethics/research-review/interventions-algorithmic-equity/" rel="alternate" type="text/html" title="Interventions for Algorithmic Equity" /><published>2020-11-26T19:00:00-05:00</published><updated>2020-11-26T19:00:00-05:00</updated><id>http://localhost:4000/ai-ethics/research-review/interventions-algorithmic-equity</id><content type="html" xml:base="http://localhost:4000/ai-ethics/research-review/interventions-algorithmic-equity/">&lt;h1 id=&quot;toward-situated-interventions-for-algorithmic-equity-lessons-from-the-field&quot;&gt;Toward Situated Interventions for Algorithmic Equity: Lessons from the Field&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Technical interventions to reduce algorithmic bias are incomplete without community-based interventions that focus on the socio-political context of technical systems.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Publisher: Katell, M. et al. Toward Situated Interventions for Algorithmic Equity: Lessons from the Field. FAT ‘20, January 27–30, 2020, Barcelona, Spain.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;I haven’t read much on non-technical means of assuring algorithmic fairness, so on first blush I think this will be an interesting contribution to the field. This work is intended to fill the gap in algorithmic bias research surrounding social-context-based means of reducing harms from algorithmic systems. The authors use the framework of situated knowledge to study technical systems. Situated knowledge refers to the “product of embodied vantage points, located historically.”&lt;sup&gt;1&lt;/sup&gt; The authors also incorporate elements of the human-computer interaction field to contextualize their participatory discourse, and draw on their experiences
developing the &lt;a href=&quot;https://www.aclu-wa.org/AEKit&quot;&gt;Algorithmic Equity Toolkit&lt;/a&gt;. The authors document their development of the Algorithmic Equity Toolkit as a response to the lack of safeguards against algorithmic bias in the 2017 Seattle Surveillance Ordinance. Participatory design methods such as contextual inquiry were used to center affected populations in the development of the toolkit. Throughout the process, the authors partnered with community-based civil rights organizations and data science institutes for their input. The toolkit, which was designed to assess the impact of particular technologies in public settings, has three tenets: (1) Determine whether a system is AI, (2) Ask tough questions, (3) Better learn how ML works and when it fails. Stakeholder-provided input facilitated continuous integration / continuous development creation  of the toolkit. Among other takeaways, the authors found that non-technical measures can meaningfully impact
algorithmic accountability. Stakeholders don’t need to have a technical background to assess outcome(s) of and identify alternatives to technical systems. The authors conclude that future automated decision system interventions should consider the framework of situated context.&lt;/p&gt;

&lt;h2 id=&quot;my-takeaways&quot;&gt;My Takeaways&lt;/h2&gt;

&lt;p&gt;The authors believe that participatory methods that center those most disparately affected by algorithmic systems should be integrated in the practice of data science. And I agree, I think it’s crucial for data scientists to recognize that the tools they build, even seemingly divorced from any social context, can prop up social inequities. On the toolkit itself, tenets (2) and (3) seem duplicative, and tenet (3) might be better listed as “understand how &lt;strong&gt;this particular&lt;/strong&gt; ML system works.” Another important takeaway is that it’s not enough to identify the risks associated with automated decision systems. Even if the system works as intended without any disparate impacts, it may still produce undesirable results (e.g., facial recognition surveillance systems).&lt;/p&gt;

&lt;p&gt;Overall, this research provides a blueprint for the “interpretative data scientist”, that is, a data scientist who is deeply ingrained with community organizations, is attuned to the needs of vulnerable populations and the potentially negative impacts of the technical systems being developed, and is an expert communicator of not just technical topics, but also understanding how those topics are situated within a socio-political context. I am hopeful that these skills increasingly become part of data science training.&lt;/p&gt;

&lt;h2 id=&quot;notable-quotes&quot;&gt;Notable Quotes&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;“A more inclusive data science practice will result in novel conventions of work which attend to fairness, accountability, transparency, and equity as an explicit part of research method and practice– rather than as topic alone.”&lt;/li&gt;
  &lt;li&gt;“…such efforts emphasize the importance of equitable processes determinative of equitable outcomes.”&lt;/li&gt;
  &lt;li&gt;“…the most meaningful measures towards fairness, accountability, and transparency are not necessarily technical but are instead informed through contextualized understanding about how a given tool or technology is implemented and contested in practice.”&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;citations&quot;&gt;Citations&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Donna Haraway. 1988. Situated knowledges: The science question in feminism and the privilege of partial perspective. &lt;em&gt;Feminist studies&lt;/em&gt; 14, 3 (1988), 575–599.&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><category term="ai-ethics" /><category term="research-review" /><summary type="html">Toward Situated Interventions for Algorithmic Equity: Lessons from the Field</summary></entry><entry><title type="html">Getting Started</title><link href="http://localhost:4000/intro/welcome/" rel="alternate" type="text/html" title="Getting Started" /><published>2020-11-22T19:00:00-05:00</published><updated>2020-11-22T19:00:00-05:00</updated><id>http://localhost:4000/intro/welcome</id><content type="html" xml:base="http://localhost:4000/intro/welcome/">&lt;p&gt;&lt;img src=&quot;/images/coffee-pic.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For about two years now, I have been deeply interested in the fields of AI ethics. I’ve been a data scientist for about four years, and my most meaningful work has been in the “data science for good” space. As I was getting more involved in the data science community, I noticed practitioners place an especially strong emphasis on how we can use big data and machine learning to advance social progress. But what I’ve come to realize is more important is recognizing and mitigating the harms that data science can cause when implemented carelessly. I think it’s incumbent upon data scientists think of themselves as political actors whose decisions have wide-ranging impacts, even on projects that begin with positive intentions.&lt;/p&gt;

&lt;p&gt;My interest in the AI ethics field was galvanized by the groundbreaking ProPublica &lt;a href=&quot;https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing&quot;&gt;COMPAS report&lt;/a&gt; that demonstrated racial bias in the recidivism predictions employed by state government. Though I was developing and implementing machine learning models, I was not thinking about opportunities for harm in high-impact areas. What moved me the most was the prospect of disparate impact at scale; an automated decision system with wide reach can pose a greater threat than a human making the same decision(s). Bias in ML compounded on my interest in dicrimination in labor economics and more generally in my social justice advocacy work.&lt;/p&gt;

&lt;p&gt;As I read more articles documenting algorithmic bias in situations comical (the Tay chatbot) and somber (predictive policing), I became more certain that I wanted to transition my career towards designing safe, fair ML systems and policy that ensures those systems are fair when companies cannot be trusted to self-regulate. By the time the ProPublica article came out, I was already years behind the curve. The Fairness, Accountability, and Transparency in Machine Learning (FAccT) community has been active since the early 2010s. I’ve read dozens of articles but still feel ambiguity about which area of fair ML research most aligns to my interests. A trip to the 2019 NeurIPS AI for Social Good workshop cemented that the field was a thriving space with many areas to contribute. So how could I make the most positive impact using my skills and background?&lt;/p&gt;

&lt;p&gt;I am developing this blog as a way to sort my thoughts on emerging AI fairness research. My goal is to better understand the state of the field and narrow down my interest areas enough that I have ledes for future research. I’ve got a soft goal of applying for a doctoral program in a field that would enable me to study this topic deeper, and I don’t want to embark on that journey without a firm footing in my research interests. This blog will summarize news articles, talks, documentaries, and research papers both so that I can get a firmer grasp on where I fit into the field, and so that others have this resource if they want to learn more about AI fairness. I am hopeful that I will update this blog on a regular basis (~1x a week) with a goal of increasing post frequency when I develop a cadence. By this time next year, maybe I’ll be more of an expert on this topic. I’ll at least hope to bring more to the table in responsible AI discussions at work!&lt;/p&gt;</content><author><name></name></author><category term="intro" /><summary type="html"></summary></entry></feed>