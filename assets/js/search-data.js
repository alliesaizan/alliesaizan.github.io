var store = [{
        "title": "Getting Started",
        "excerpt":" For about two years now, I have been deeply interested in the fields of AI ethics. I’ve been a data scientist for about four years, and my most meaningful work has been in the “data science for good” space. As I was getting more involved in the data science community, I noticed practitioners place an especially strong emphasis on how we can use big data and machine learning to advance social progress. But what I’ve come to realize is more important is recognizing and mitigating the harms that data science can cause when implemented carelessly. I think it’s incumbent upon data scientists think of themselves as political actors whose decisions have wide-ranging impacts, even on projects that begin with positive intentions. My interest in the AI ethics field was galvanized by the groundbreaking ProPublica COMPAS report that demonstrated racial bias in the recidivism predictions employed by state government. Though I was developing and implementing machine learning models, I was not thinking about opportunities for harm in high-impact areas. What moved me the most was the prospect of disparate impact at scale; an automated decision system with wide reach can pose a greater threat than a human making the same decision(s). Bias in ML compounded on my interest in dicrimination in labor economics and more generally in my social justice advocacy work. As I read more articles documenting algorithmic bias in situations comical (the Tay chatbot) and somber (predictive policing), I became more certain that I wanted to transition my career towards designing safe, fair ML systems and policy that ensures those systems are fair when companies cannot be trusted to self-regulate. By the time the ProPublica article came out, I was already years behind the curve. The Fairness, Accountability, and Transparency in Machine Learning (FAccT) community has been active since the early 2010s. I’ve read dozens of articles but still feel ambiguity about which area of fair ML research most aligns to my interests. A trip to the 2019 NeurIPS AI for Social Good workshop cemented that the field was a thriving space with many areas to contribute. So how could I make the most positive impact using my skills and background? I am developing this blog as a way to sort my thoughts on emerging AI fairness research. My goal is to better understand the state of the field and narrow down my interest areas enough that I have ledes for future research. I’ve got a soft goal of applying for a doctoral program in a field that would enable me to study this topic deeper, and I don’t want to embark on that journey without a firm footing in my research interests. This blog will summarize news articles, talks, documentaries, and research papers both so that I can get a firmer grasp on where I fit into the field, and so that others have this resource if they want to learn more about AI fairness. I am hopeful that I will update this blog on a regular basis (~1x a week) with a goal of increasing post frequency when I develop a cadence. By this time next year, maybe I’ll be more of an expert on this topic. I’ll at least hope to bring more to the table in responsible AI discussions at work! ","categories": ["intro"],
        "tags": [],
        "url": "http://localhost:4000/intro/welcome/"
      },{
        "title": "Interventions for Algorithmic Equity",
        "excerpt":"Toward Situated Interventions for Algorithmic Equity: Lessons from the Field Technical interventions to reduce algorithmic bias are incomplete without community-based interventions that focus on the socio-political context of technical systems. Publisher: Katell, M. et al. Toward Situated Interventions for Algorithmic Equity: Lessons from the Field. FAT ‘20, January 27–30, 2020, Barcelona, Spain. Summary I haven’t read much on non-technical means of assuring algorithmic fairness, so on first blush I think this will be an interesting contribution to the field. This work is intended to fill the gap in algorithmic bias research surrounding social-context-based means of reducing harms from algorithmic systems. The authors use the framework of situated knowledge to study technical systems. Situated knowledge refers to the “product of embodied vantage points, located historically.”1 The authors also incorporate elements of the human-computer interaction field to contextualize their participatory discourse, and draw on their experiencesdeveloping the Algorithmic Equity Toolkit. The authors document their development of the Algorithmic Equity Toolkit as a response to the lack of safeguards against algorithmic bias in the 2017 Seattle Surveillance Ordinance. Participatory design methods such as contextual inquiry were used to center affected populations in the development of the toolkit. Throughout the process, the authors partnered with community-based civil rights organizations and data science institutes for their input. The toolkit, which was designed to assess the impact of particular technologies in public settings, has three tenets: (1) Determine whether a system is AI, (2) Ask tough questions, (3) Better learn how ML works and when it fails. Stakeholder-provided input facilitated continuous integration / continuous development creation  of the toolkit. Among other takeaways, the authors found that non-technical measures can meaningfully impactalgorithmic accountability. Stakeholders don’t need to have a technical background to assess outcome(s) of and identify alternatives to technical systems. The authors conclude that future automated decision system interventions should consider the framework of situated context. My Takeaways The authors believe that participatory methods that center those most disparately affected by algorithmic systems should be integrated in the practice of data science. And I agree, I think it’s crucial for data scientists to recognize that the tools they build, even seemingly divorced from any social context, can prop up social inequities. On the toolkit itself, tenets (2) and (3) seem duplicative, and tenet (3) might be better listed as “understand how this particular ML system works.” Another important takeaway is that it’s not enough to identify the risks associated with automated decision systems. Even if the system works as intended without any disparate impacts, it may still produce undesirable results (e.g., facial recognition surveillance systems). Overall, this research provides a blueprint for the “interpretative data scientist”, that is, a data scientist who is deeply ingrained with community organizations, is attuned to the needs of vulnerable populations and the potentially negative impacts of the technical systems being developed, and is an expert communicator of not just technical topics, but also understanding how those topics are situated within a socio-political context. I am hopeful that these skills increasingly become part of data science training. Notable Quotes   “A more inclusive data science practice will result in novel conventions of work which attend to fairness, accountability, transparency, and equity as an explicit part of research method and practice– rather than as topic alone.”  “…such efforts emphasize the importance of equitable processes determinative of equitable outcomes.”  “…the most meaningful measures towards fairness, accountability, and transparency are not necessarily technical but are instead informed through contextualized understanding about how a given tool or technology is implemented and contested in practice.”Citations   Donna Haraway. 1988. Situated knowledges: The science question in feminism and the privilege of partial perspective. Feminist studies 14, 3 (1988), 575–599.","categories": ["ai-ethics","research-review"],
        "tags": [],
        "url": "http://localhost:4000/ai-ethics/research-review/interventions-algorithmic-equity/"
      },{
        "title": "Lessons from Archives",
        "excerpt":"Lessons from Archives: Strategies for  Collecting Sociocultural Data in Machine Learning What can data scientists learn from the archival sciences about data collection? Publisher: Conference on Fairness, Accountability, and Transparency (FAT ’20), January 27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3351095.3372829 Summary Reader, you are likely already aware that haphazardly-collected data can propagate social biases. Most ML research focused on devising state-of-the-art models and benchmarks does not critically examine the data sources used to develop models and benchmarks. Data curation as a whole is not a part of most ML research portfolios. The authors propose an alternate approach to interrogating data informed by archival research best practices. Further, they advocate for the creation of an interdisciplinary field whose mission is “data gathering, sharing, annotation, ethics monitoring, and record-keeping processes.” The authors focus on archives because they have many foci that parallel the needs of data curation in ML, including full-time curators, community participation, standardized documentation methods, and professional codes of ethics. Fundamentally, archival science can offer extensive study on the extraction of human information. Archives represent the other extreme of the data collection spectrum, interventionist where ML is unscrupulous. But the authors argue that adapting a more interventionist approach to data collection can help researchers avoid replicating historical and representational biases. A list of proposed integrations from archival science that stood out to me include:   Data consortia, which would set up institutional frameworks for ethical data collection to benefit both small and large firms.  Resources that ensure transparency, such as Datasheets for Datasets, which record not only the contents of data, but also the process of data collection.  A code of ethics for data collection with appropriate incentive measures to ensure compliance.The authors conclude with a macro-list of action items:   Congregate and develop data consortia  Establish professional organizations that work by member-ship to enforce adherence to ethical guidelines  Support community archives  Develop a subfield dedicated to the data collection and an-notation processMy Takeaways The idea of treating data collection carefully is not new to me; in economics research, generally an entire section of a paper details data collection methods and the overarching statistical properties of the data. I also have experience with archival research through a research project I completed in college. However, this paper has made me realize that just looking at superficial metadata like summary statistics is woefully inadequate. The process for data curation outlined here is so thorough and thoughtful, and I genuinely want to live in a world where data collection is treated with more reverence like explicated in this research. I’m particularly struck by this recommendation to the data science community: “form or integrate existing global/national organizations in instituting standardized codes of ethics/conduct and procedures to review violations.” So often, when I think about ML ethics, it’s hard for me to envision a sufficient incentive structure. I think the authors succeed in depicting the archival model, or something more like it, as a means of safeguarding ML models. ","categories": ["ai-ethics","research-review"],
        "tags": [],
        "url": "http://localhost:4000/ai-ethics/research-review/lessons-from-archives/"
      },{
        "title": "From What to How",
        "excerpt":"From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices A 2019 review of AI ethics tools and methods Publisher: Morley, J., Floridi, L., Kinsey, L. et al. From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices. Sci Eng Ethics 26, 2141–2168 (2020). https://doi.org/10.1007/s11948-019-00165-5 Summary The authors begin by asserting that code, and particularly code that facilitates machine learning, is both our greatest threat and our greatest promise. Citing a review of AI ethics principles, the argues note that ethically-aligned machine learning is typically defined as having the following properties: beneficence, non-maleficence, autonomy, justice, and explainability. This consensus on the definition of AI creates a foundation upon which technologists can “communicate expectations and evaluate deliverables”. However, very few of these guidelines set technical expectations. The authors note that the next step in defining ethical machine learning guidelines is to translate the what outlined in the principles to how. To bridge this gap, the authors define a typology that maps the foundational principles above to each stage of the machine learning algorithm design process. They identified 425 papers which ostensibly answer the question of “how to develop an ethical algorithmic system”. The typology developed is intended to show machine learning developers what ethical ML tools are already available. The authors find that not all principles have tools for each stage of the algorithm design process. Explainability, which is more easily implemented than the other principles, is overrepresented in the toolset. Unlike beneficence, non-maleficence, autonomy, and justice, explainability is not a moral principle, so tools that enable transparency like LIME and SHAP are insufficient alone. Instead of ensuring explainability at the back end, the authors propose that transparency is prioritized at the beginning of the design process. The authors also note that few of the tools provide ways to assess the impact of an ML algorithm on an individual in a dataset. The deployment column of the typology is blank, highlighting the need for “pro- ethically designed human-computer interaction (at an individual level) or networks of ML systems (at a group level)”. The authors assert that not enough research has been done in the field of translating predictions to decisions in the context of algorithmic systems. Finally, the tools in the typology are generally not usable, as they are either not easily applicable in practice or too complex for a wide set of users, as in the case of open source code libraries. ML researchers from all disciplines will need to accept that: (1) AI is built on assumptions; (2) human behaviour is complex; (3) algorithms can have unfair consequences; (4) algorithmic predictions can be hard to interpret (Vaughan &amp; Wallach, 2016); (5) trade-offs are usually inevitable; and (6) positive, ethical features are open to progressive increase. Ethical principles should not be applied once or not, but should be regularly re-applied or applied differently, depending on changing needs throughout the ML development process. My Takeaways I don’t think this article provided me with anything I didn’t already know. Through my (albeit limited) review of AI ethics principles, it’s evident that a wide gap exists between framework and technical practice. The guidelines I tend to see fall on one of two extremes: high-level principles that outline the values algorithmic systems should embed, or low-level technical implementations of those values (such as transparency as in SHAP, or fairness as in fairlearn) without explicating a context for prioritizing those results within organizational and/or project goals. I appreciated that the authors’ literature review and typology reflected this sentiment, and that they provided a thorough synthesis of the existing ML ethics conversation. I think one of my most important takeaways is at the end of the article, where the authors describe what topics the ML research community prioritize. They put the most important topic at the bottom of the list: the evaluation and creation of pro-ethical business models and incentive structures that balance the costs and rewards of investing in ethical AI across society. I firmly believe that none of the other topics can be on the table, not a commitment to reproducibility and openness, not evaluation of currently implemented tools, not the development of a common language, etc. until ML developers have organizational buy-in. Organizational buy-in only occurs when ethical priorities align with financial priorities, and in some cases the only way to facilitate that alignment may be a fundamental restructuring of financial priorities. The authors note that they eventually hope to create a searchable database with AI ethics tools and methods, and I think that would be particularly helpful to practitioners. The authors note an urgent need to progress research in the AI for Social Good field, and having a comprehensive database of tools (with a common/standardized language to discuss concepts) would be a good first step to making that research feasible. Notable Quotes Citations ","categories": ["ai-ethics","research-review"],
        "tags": [],
        "url": "http://localhost:4000/ai-ethics/research-review/review-of-tools/"
      },{
        "title": "Mitigating Dataset Harms Requires Stewardship",
        "excerpt":"Mitigating Dataset Harms Requires Stewardship: Lessons from 1000 Papers Advocating for a distributed approach to dataset harms mitigation Publisher: Kenny Peng, Arunesh Mathur, and Arvind Narayanan. “Mitigating Dataset Harms Requires Stewardship: Lessons from 1000 Papers.” 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks (2021). Summary The author’s approach is to evaluate the life cycles of three datasets that have recently faced ethical scrutiny: Labeled Faces in the Wild (LFW), MS-Celeb-1M, andDukeMTMC. These datasets are used for face/person recognition tasks. After analyzing nearly 1,000 papers that cite using one or more of these datasets, the authors made the following findings:   Dataset retraction has a limited effect on mitigating harms  Licenses can be ineffective at restraining production use of datasets  Social and technological change can change the ethical concerns associated with a dataset  Current dataset management and citation practices have shortcomings that means they are sometimes ineffective at ensuring ethical use.Dataset retractions work on limiting the use of datasets by both making it unavailable and setting normative expectations around use. However, in the case of the datasets the author studied, formal retraction did not make much of a difference because the dataset was also available in other, informal places. Another way others can learn about retracted datasets is through the citation of retracted papers, which the authors note is a problem in the machine learning field. Derivatives of datasets, which are used for purposes outside the intentions of the dataset creators, can also raise concerns. Derivatives can include updates with relabeled features, models trained on those datasets, and other post-processing modifications meant to improve performance on the original task. The authors also found that non-commercial dataset licenses, like those used by the datasets analyzed, are sometimes ignored in practice. The licenses either do not prohibit distribution or are no longer available. The authors note that ethical norms have shifted between when many popular datasets were first published and the current day. Most egregiously, in the case of ImageNet, researchers discovered misogynistic and racist slurs in image labels nearly a decade after the dataset’s release, prompting the dataset maintainers to remove those images. Finally, the authors find that none of the datasets they studied were centrally managed or have stable identifiers for tracking purposes. The authors put forward a number of recommendations for dataset creators to mitigate potential harms:   Make licenses and dataset documentation clear and easily accessible  Maintain active stewardship over datasets to address ethical concerns as they arise  Conferences should take an active role in encouraging responsible dataset use, advocating for dataset management and citation practices, and incentivizing responsible dataset creation through new publishing avenuesMy Takeaways My main takeaway is that I am not surprised the authors found such glaring issues with popular benchmarking datasets. They note at one point that “creating datasets is already an undervalued activity”, so why would dataset creators feel incentivized to be better stewards of their data? I recently wrote a blog post about the significant fairness issues associated with another non-image recognition benchmarking dataset (Boston Housing), and many of the same problems abounded (included unclear documentation, no licenses, shifting ethical norms, etc). That dataset was prolific in machine learning benchmarking, and often in applications that stray far from the original intended use of the data. I think ultimately this phenomenon is a consequence of the academic publish-or-perish cycle. This cycle has created an environment where cutting edge machine learning algorithms (or slight improvements on them) are needed to pass publisher muster, and simultaneously there are no official datasets allocated for machine learning testing. So popular datasets become the norm for benchmarking with little attention paid to the potential impacts of using a given algorithm on those data. I think the authors’ recommendations are good first steps, and chief among them should be restructuring the publishing incentive structure (as noted by their recommendations around conferences). If dataset evaluation research is rewarded, people will do it! I consider the second-most important recommendation to be the establishment of a centralized data management structure/review body. As the authors note, Institutional Review Boards may be insufficient for this new class of ethical concerns, and, not to be too cliche, modern problems require modern solutions. I think back to an article I read some time ago about dataset archives as a possible centralized data management option - it’s interesting that the authors did not review that suggestion in their analysis. Overall, I have been seeing more field surveys out of arXiv, and I think that’s good! More holistic reviews of datasets in particular will only further the development and adoption of new ethical behavior norms. ","categories": ["ai-ethics","research-review"],
        "tags": [],
        "url": "http://localhost:4000/ai-ethics/research-review/mitigating-dataset-harms/"
      },{
        "title": "Fairness Issues as a Consequence of Technical Debt",
        "excerpt":"Fairness Issues as a Consequence of Technical Debt I recently transitioned off a client project involving the development of a large machine learning prediction system. Unfortunately for me, we didn’t get to the long-coveted deployment stage before project funding wore out. During my retrospection about how the project went and ways I could improve on my next machine learning project, I was thinking about the hurdles the client might encounter if they tried to deploy the project at some point in the future when I am not there. Although I documented my work thoroughly, the client will likely still encounter gaps in information transfer, broken commands or links, and unexpected results from data processing scripts. The accumulation of these challenges, which can be categorized under a concept called technical debt, is important to manage during any machine learning project. I began to realize that defects outside of traditional model deployment can also be classified under technical debt: things like machine bias and model opaqueness. This realization sparked my curiosity: how might these risks differ from traditional software development risks, or even risks specific to production machine learning systems, and how can technical debt management reframe the way we mitigate fairness and transparency risks? First, let’s start off by defining technical debt. Technical debt, defined as “not quite right code which we postpone making it right,” is a concept introduced by Ward Cunningham in 1992.[1] Its original purpose was to provide a rationale for what we now call code refactoring, and includes the long-term costs incurred by short-term development in software engineering. It can be thought of as analogous to financial debt, where the extra effort to add new features to a software program is like interest on a debt.[2] Like paying off financial debt, a high-level technical debt management strategy is to “pay off” the “principal” before the “interest”; that is, addressing software modules with internal quality deficiencies, spending more time on areas of code that are modified more frequently, to lessen the cost (the “interest”) of modifying those modules in the future. Software quality improvement includes actions like refactoring code, improving unit tests, and improving documentation.[3] The ultimate goal of technical debt management is to improve the ability of systems to be maintained over time without significant interruptions in service. The concept of technical debt has been expanded over the years with the help of new tools[5] and taxonomies[4]. A 2012 IEEE Software article asserts that technical debt may not involve code, but may be the result of architectural or structural choices or technological gaps.[6] This statement is especially true of machine learning systems, which present risks separate from traditional software engineering. In a machine learning system, technical debt can manifest in a number of ways. Below, I list the technical debt risks most relevant to FATE (fairness, accountability, transparency, explainability). These risks were proposed by Sculley et al (2015).[3]   Unstable data dependency: If input data is unstable, then the behavior of a machine learning system using that data can change over time in unpredictable ways. Examples of unstable data dependencies are mappings whose key-value pairs change over time and mis-calibrated data encodings that are later corrected. One method of mitigating this risk is to use data versioning to encapsulate stable data extracts.  Underutilized data dependency: Features that are not needed in the input data, such as legacy features no longer in use or correlated features, can impact a model’s predictive power. The authors recommend using leave-one-feature-out evaluation methods to identify unneeded features.Feedback loops: Feedback loops occur when machine learning systems influence their own behavior as they update over time. They can be direct or hidden. Iin the hidden case, two systems influence each other in the world (think of a machine learning system that predicts which products to show, and another that predicts product reviews). Feedback loops are difficult to diagnose and remedy.  Pipeline instability: A typical machine learning workflow contains code that manipulates data using operations like joins, sampling, filters, et cetera, sometimes generating intermediate file outputs. If these pipelines are created ad hoc, then they may generate bugs and failures that are difficult to properly diagnose and mitigate. Pipelines are a special case of what the authors call “glue code”, which is often opaque utility code crafted to facilitate data preparation tasks. To mitigate pipeline instability, the authors recommend that engineers and researchers work together to develop ML packages to reduce the risk that packages appear to be black boxes.  Protype smell: Prototypes can be useful for initial model development, but small-scale results may not accurately reflect the phenomena the system seeks to measure at full-scale.  Fixed thresholds in dynamic systems: If the distribution of new data changes from the original data used to develop a ML system, then previously established thresholds may no longer be valid for achieving the desired model performance.  Prediction bias: In the supervised learning case, this bias occurs when the distribution of predicted labels is not equal to the distribution of observed labels and can be detected via automated testing.These risks directly map to FATE concerns. For ensuring transparency and explainability of ML systems, pipeline instability can make it difficult or impossible for others outside the engineering team to successfully run ML packages. In an ad-hoc data preparation pipeline, it may not be clear what intermediate outputs should be expected, and the order in which steps should be run. This opaqueness reduces the pipeline’s reproducibility and consequently renders it less useful over time as the ML package is maintained by different engineers on the team and encounters distributional data shifts. Additionally, hidden feedback loops may make it difficult to parse ground truth data-generating processes and understand how the ML system should be behaving versus its actual behavior. The technical debt risks above also present fairness risks. Unstable data dependencies can mean that if the data for a minority group changes more frequently than for the majority group, the ML system may have lower or more unstable predictive power for minority groups over time. Underutilized data dependencies, like legacy features, may represent historical patterns of discrimination that should no longer be reflected in the data. An example of a legacy feature that may impose fairness-related harms is outdated racial classifications containing terms for minority groups that reflect historical social prejudice. Outdated classifications may introduce unwanted signals of societal bias into the data. Correlated features are of significant concern in mitigating fairness-related harms, as they may encapsulate the signals of minority group data excluded from the input dataset (a famous example of a correlated but presumed innocuous feature is zip code, which can encapsulate societal racism due to the history of redlining). Feedback loops also can present fairness concerns, as it may not be clear how an indirectly-related system impacts a model’s predictive power for minority groups. Prototype development, fixed thresholds, and prediction bias are all impacted by the dynamic nature of machine learning systems. A prototype dataset may not be representative of all demographic groups due to missingness, which may translate to poorer performance for those groups when the prototype model is deployed to production and has access to the full-scale data. Fixed thresholds and the discrepancy between predicted and output labels may also be more variable for minority than majority groups if those groups are not properly accounted for during data preparation. Dynamic system risks require diligent, automated model and data monitoring, data versioning, and careful design of models and datasets to anticipate and address potential fairness-related harms. How might we use the framework of technical debt to tackle fairness and explainability issues? A popular management strategy is improving the integrity and quality of the original software architecture and code base to make modifications and additions easier in the future. For mitigating FATE concerns, this strategy includes ensuring the original ML system itself is fair, transparent, and secure, and that these factors take precedence when considering additions or modifications to the system. FATE analogues for common technical debt resolution strategies might include:   Refactoring code: Refactoring is the process of restructuring existing code to improve its design or structure while preserving its external behavior. Reducing code complexity, including by prioritizing glassbox or explainable models during development, can make the logical steps within easier to follow and thus facilitate transparency and explainability.  Improving tests: Improving testing across the board is one way to catch disparities in outcomes both before models are deployed and after they are running in a production environment. As well as unit testing small-scale results (such as asserting that predictions for a minority group are within some threshold of predictions for the majority group), ML monitoring systems should also include fairness-specific evaluation metrics. In a monitoring scenario, engineers would set the acceptable thresholds for metrics like demographic parity, equality of odds, etc, and configure alerts for when the metric values fall outside of the threshold.  Reducing dependencies: Data quality has a significant impact on the fidelity of ML systems. Obviously enough, reducing data dependencies is one way to bolster data quality. Prioritizing data with stable input labels and versioning data during the data manipulation process can mitigate instability in model results. As discussed above, legacy and correlated features may reflect patterns of social bias that ML models should not capture. Taking care to identify and remove these fields can reduce the chance that embedded social biases end up in downstream model results.  Improving documentation: Robust documentation makes ML systems easier to maintain over time. Documentation frameworks like Model Cards for Model Reporting[7] and Datasheets for Datasets[8] give data science teams transparency into the model development process that persists across staff turnover.I’d like to pause here to note that fairness issues are fundamentally socio-technical challenges, and technical approaches to mitigation may be insufficient for a given use case. In general, software tools can mitigate harms, but cannot solve them. Before ML software development begins, the most useful harms mitigation strategy may be to ask whether ML is appropriate for solving the problem at hand. A ML system that is inappropriately applied to a social problem presents significant risks outside of technical debt. The technical debt risk mitigation strategy leverages a holistic approach to assuring software is maintainable over time. When machine learning is a core part of a software system, that holistic approach must include considering if the effort of developing a potentially high-risk system and deploying it in a dynamic environment is worth the potential harms it may engender to system stakeholders. Thanks for reading this post! Citations   W. Cunningham, “The WyCash Portfolio Management System,” Proc. OOPSLA, ACM, 1992; http://c2.com/doc/oopsla92.html.  M. Fowler, “Technical Debt,” blog, 2009; http://martinfowler.com/bliki/TechnicalDebt.html.  D. Sculley et al, “Hidden Technical Debt in Machine Learning Systems”, NeurIPS, 2015.  I. Gat, ed., “Special Issue on Technical Debt,” Cutter IT J., vol. 23, no. 10, 2010.  M. Fowler, “TechnicalDebtQuadrant,” blog, 2009; https://martinfowler.com/bliki/TechnicalDebtQuadrant.html.  P. Krutchen, R. Nord, I. Ozkaya, “Technical Debt: From Metaphor to Theory and Practice”, IEEE Magazine, 20212.  M. Mitchell et al, “Model Cards for Model Reporting”, FAT*, 2019; https://www.seas.upenn.edu/~cis399/files/lecture/l22/reading2.pdf.  T. Gebru, et al, “Datasheets for Datasets”, Proceedings of the 5th Workshop on Fairness, Accountability, and Transparency in Machine Learning, 2018; https://www.microsoft.com/en-us/research/uploads/prod/2019/01/1803.09010.pdf.","categories": ["ai-ethics","original-writing"],
        "tags": [],
        "url": "http://localhost:4000/ai-ethics/original-writing/technical-debt-fairness/"
      },{
        "title": "AITA Classifier",
        "excerpt":"A BERT-based text classifier! Table of Contents   Overview  How to run this experiment on your machine  Development process  Lessons learnedOverview  Welcome! This application is a small experiment that uses the BERT large language model to classify posts from the Am I The Asshole (AITA) subreddit. The model used to generate predictions is actually a small, fast, “distilled” version of the BERT model meant for finetuning ondownstream tasks. Please note, some of the files needed to be zipped to accomodate Git Large File Storage. The raw data folder and trained model are zip files. You can find the original project structure in my OneDrive folder (view-only!). You can also view this project on GitHub. My motivation for this project was to familiarize myself with the HuggingFace library. I’ve worked with LLMs in personal projects before, and wanted to experiment with the state-of-the-art LLM. I chose AITA data because:   Okay, I love reading those stories. Sometimes, there’s just no better way to spend time on the Internet than marveling at the extent of other peoples’ audacity 😅  The “Asshole/Not an Asshole” dichotomy was easy to translate into a text classification problem. There are other post categories, like “Everyone Sucks Here”, that indicates the situation is more complex, but I don’t include those posts in this analysis.This repository is structured as follows:   data          raw                  big-query-aita-aug18-aug19.zip – The results of a SQL query run against a database of reddit posts. The dataset covers all AITA posts from August 2018-2019. Unzip this file to use it.                      results          checkpoint-20238.zip –The final PyTorch text classification model, saved at the last epoch runtime. Unzip this file to use it.        src          transformers-model-train.py – Python file used to train the model. Uses CUDA on GPU        static          icon.png – The AITA header icon      style.css – webpage formatting        templates          home.html – Homepage      results.html – Webpage for prediction results        Dockerfile – specifies container construction  app.py – a Flask application that enables the user to enter a AITA post and classifies whether the original poster (OP) is an asshole or not  docer_compose.yaml – Docker-compose file that specifies volume construction  requirements_docker.txt – Python libraries used to run the projectHow to run this experiment on your own machine  Clone this repository to your local machine and open up the command prompt (Windows) or terminal (Mac, Linux). You will need to download the distilled BERT model to the working repository, as Docker-compose will include this folder as a volume. You can download the BERT model by running the command git clone https://huggingface.co/distilbert-base-uncased; note that it’s about 2.4GB in size so download may take a while! Ensure Docker is installed on your machine. For Windows, docker-compose will come bundled in the installation. Run docker-compose up -d --build as a command line operation. The Docker container will build and begin running. You can then navigate to port 5000 on your local machine to view the app! Development process  I obtained the data I used to finetune the model from Google Cloud’s BigQuery service. As mentioned above, the data consists of posts from the AITA subreddit with the flair “Asshole” or “Not the Asshole”. I removed records where the body of the post was blank or deleted (I wonder how Reddit users assess these posts anyways!). The dataset consists of about ~72,000 posts after removing these records. I followed HuggingFace’s text classification tutorial. Subsequent data preparation tasks include splitting the data into training and tests sets (I used a 75% split), tokenizing the text using the  AutoTokenizer with BERT as the base model, and transforming the training and test sets into a Dataset object. I then defined a Data Collator, which creates a bath of examples for the model to process in each step. The DataCollatorWithPadding class also ensures each text element in a batch is of uniform length. I defined the training hyperparameters to be used during the finetuning process, specifically:   Learning rate: 2e-5-Batch size for each device: 4  Number of training epochs: 3  Evaluate results at each epoch: true  Save results at each epoch: trueThen it was model training time! Using a GPU, the model training process completed in about 2.5 hours. The training process outputted checkpoint models to the /results directory, and I used the model outputted in the last epoch as my final model. When model training was complete, I began to build the accompanying webapp. The home page, shown below, instructs the user to paste in the text body of an AITA post. When the user hits submit, the app will redirect to the “results” page, which will either output an prediction or will instruct the user to try a different post. Longer posts tend to generate more prediction errors; I didn’t investigate this in the course of development but would revisit this issue in future iterations.  Finally, because just building the final model and app wasn’t enough, I also wanted to take this opportunity to learn more about using Docker. I specified the configuration of the container in the Dockerfile and used the docker-compose command to set the BERT model up as a volume, build the container, and run it. Lessons learned  I learned quite a lot in the process of developing this application, including:   How HuggingFace’s API works  How to successfully train a model on GPU  How to deploy a model using a Flask app  How to deploy that app as a containerThe tutorial I used data from HuggingFace’s datasets module. If you want to use a custom dataset, you’ll need to preprocess the data for the downstream transformers classes. Figuring out the correct data format took some trial and error. This post on custom datasets was somewhat helpful, but I eventually figured out that the Dataset class needed records to be in a dictionary of the form {key: Torch tensor of text encodings at each index, labels: Torch tensor of the labels at each index}. Other text classification packages I’ve used, like gensim and NLTK, required more minimal data preprocessing, but the process of transforming the data manually did give me a little more insight into how the HuggingFace transformers infrastructure works. I spent some time trying to figure out how to configure CUDA on my 64-bit Windows computer. I do not have much experience with deep learning models, and in the past I haven’t really needed to use my GPU to train simpler (by comparison) clustering or ensemble models. I was able to figure out how to train models on my GPU after concluding that my initial PyTorch and transformers installations were incorrect. However, I ultimately found it more efficient to train the model on a GPU-enabled Google Colab notebook. Amelliorating my anxiety around running up my power bill + a shorter computing duration was certainly worth the small fee to use the notebook. Tip: GPU-enabled notebooks are available for free, but if you exceed predefined limits (by, for example, training complex deep neural networks on large datasets), you are temporarily prohibited from using GPUs to let other users run workloads. I wanted to ensure the app worked without transformers needing to use the Internet to fetch BERT each time the user runs the app. Volumes helped me figure out how to give my container access to the large BERT files through what’s essentially a virtual hard drive. Volumes don’t increase the size of the containers using them, so they’re a great option for making large datasets available to apps that should otherwise be lightweight. Thank you for reading! 🎉 ","categories": ["projects"],
        "tags": [],
        "url": "http://localhost:4000/projects/aita-classifier/"
      }]
