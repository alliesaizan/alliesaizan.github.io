var store = [{
        "title": "Getting Started",
        "excerpt":" For about two years now, I have been deeply interested in the fields of AI ethics. I’ve been a data scientist for about four years, and my most meaningful work has been in the “data science for good” space. As I was getting more involved in the data science community, I noticed practitioners place an especially strong emphasis on how we can use big data and machine learning to advance social progress. But what I’ve come to realize is more important is recognizing and mitigating the harms that data science can cause when implemented carelessly. I think it’s incumbent upon data scientists think of themselves as political actors whose decisions have wide-ranging impacts, even on projects that begin with positive intentions. My interest in the AI ethics field was galvanized by the groundbreaking ProPublica COMPAS report that demonstrated racial bias in the recidivism predictions employed by state government. Though I was developing and implementing machine learning models, I was not thinking about opportunities for harm in high-impact areas. What moved me the most was the prospect of disparate impact at scale; an automated decision system with wide reach can pose a greater threat than a human making the same decision(s). Bias in ML compounded on my interest in dicrimination in labor economics and more generally in my social justice advocacy work. As I read more articles documenting algorithmic bias in situations comical (the Tay chatbot) and somber (predictive policing), I became more certain that I wanted to transition my career towards designing safe, fair ML systems and policy that ensures those systems are fair when companies cannot be trusted to self-regulate. By the time the ProPublica article came out, I was already years behind the curve. The Fairness, Accountability, and Transparency in Machine Learning (FAccT) community has been active since the early 2010s. I’ve read dozens of articles but still feel ambiguity about which area of fair ML research most aligns to my interests. A trip to the 2019 NeurIPS AI for Social Good workshop cemented that the field was a thriving space with many areas to contribute. So how could I make the most positive impact using my skills and background? I am developing this blog as a way to sort my thoughts on emerging AI fairness research. My goal is to better understand the state of the field and narrow down my interest areas enough that I have ledes for future research. I’ve got a soft goal of applying for a doctoral program in a field that would enable me to study this topic deeper, and I don’t want to embark on that journey without a firm footing in my research interests. This blog will summarize news articles, talks, documentaries, and research papers both so that I can get a firmer grasp on where I fit into the field, and so that others have this resource if they want to learn more about AI fairness. I am hopeful that I will update this blog on a regular basis (~1x a week) with a goal of increasing post frequency when I develop a cadence. By this time next year, maybe I’ll be more of an expert on this topic. I’ll at least hope to bring more to the table in responsible AI discussions at work! ","categories": ["intro"],
        "tags": [],
        "url": "http://localhost:4000/intro/welcome/"
      },{
        "title": "Interventions for Algorithmic Equity",
        "excerpt":"Toward Situated Interventions for Algorithmic Equity: Lessons from the Field Technical interventions to reduce algorithmic bias are incomplete without community-based interventions that focus on the socio-political context of technical systems. Publisher: Katell, M. et al. Toward Situated Interventions for Algorithmic Equity: Lessons from the Field. FAT ‘20, January 27–30, 2020, Barcelona, Spain. Summary I haven’t read much on non-technical means of assuring algorithmic fairness, so on first blush I think this will be an interesting contribution to the field. This work is intended to fill the gap in algorithmic bias research surrounding social-context-based means of reducing harms from algorithmic systems. The authors use the framework of situated knowledge to study technical systems. Situated knowledge refers to the “product of embodied vantage points, located historically.”1 The authors also incorporate elements of the human-computer interaction field to contextualize their participatory discourse, and draw on their experiencesdeveloping the Algorithmic Equity Toolkit. The authors document their development of the Algorithmic Equity Toolkit as a response to the lack of safeguards against algorithmic bias in the 2017 Seattle Surveillance Ordinance. Participatory design methods such as contextual inquiry were used to center affected populations in the development of the toolkit. Throughout the process, the authors partnered with community-based civil rights organizations and data science institutes for their input. The toolkit, which was designed to assess the impact of particular technologies in public settings, has three tenets: (1) Determine whether a system is AI, (2) Ask tough questions, (3) Better learn how ML works and when it fails. Stakeholder-provided input facilitated continuous integration / continuous development creation  of the toolkit. Among other takeaways, the authors found that non-technical measures can meaningfully impactalgorithmic accountability. Stakeholders don’t need to have a technical background to assess outcome(s) of and identify alternatives to technical systems. The authors conclude that future automated decision system interventions should consider the framework of situated context. My Takeaways The authors believe that participatory methods that center those most disparately affected by algorithmic systems should be integrated in the practice of data science. And I agree, I think it’s crucial for data scientists to recognize that the tools they build, even seemingly divorced from any social context, can prop up social inequities. On the toolkit itself, tenets (2) and (3) seem duplicative, and tenet (3) might be better listed as “understand how this particular ML system works.” Another important takeaway is that it’s not enough to identify the risks associated with automated decision systems. Even if the system works as intended without any disparate impacts, it may still produce undesirable results (e.g., facial recognition surveillance systems). Overall, this research provides a blueprint for the “interpretative data scientist”, that is, a data scientist who is deeply ingrained with community organizations, is attuned to the needs of vulnerable populations and the potentially negative impacts of the technical systems being developed, and is an expert communicator of not just technical topics, but also understanding how those topics are situated within a socio-political context. I am hopeful that these skills increasingly become part of data science training. Notable Quotes   “A more inclusive data science practice will result in novel conventions of work which attend to fairness, accountability, transparency, and equity as an explicit part of research method and practice– rather than as topic alone.”  “…such efforts emphasize the importance of equitable processes determinative of equitable outcomes.”  “…the most meaningful measures towards fairness, accountability, and transparency are not necessarily technical but are instead informed through contextualized understanding about how a given tool or technology is implemented and contested in practice.”Citations   Donna Haraway. 1988. Situated knowledges: The science question in feminism and the privilege of partial perspective. Feminist studies 14, 3 (1988), 575–599.","categories": ["ai-ethics","research-review"],
        "tags": [],
        "url": "http://localhost:4000/ai-ethics/research-review/interventions-algorithmic-equity/"
      },{
        "title": "Lessons from Archives",
        "excerpt":"Lessons from Archives: Strategies for  Collecting Sociocultural Data in Machine Learning What can data scientists learn from the archival sciences about data collection? Publisher: Conference on Fairness, Accountability, and Transparency (FAT ’20), January 27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3351095.3372829 Summary Reader, you are likely already aware that haphazardly-collected data can propagate social biases. Most ML research focused on devising state-of-the-art models and benchmarks does not critically examine the data sources used to develop models and benchmarks. Data curation as a whole is not a part of most ML research portfolios. The authors propose an alternate approach to interrogating data informed by archival research best practices. Further, they advocate for the creation of an interdisciplinary field whose mission is “data gathering, sharing, annotation, ethics monitoring, and record-keeping processes.” The authors focus on archives because they have many foci that parallel the needs of data curation in ML, including full-time curators, community participation, standardized documentation methods, and professional codes of ethics. Fundamentally, archival science can offer extensive study on the extraction of human information. Archives represent the other extreme of the data collection spectrum, interventionist where ML is unscrupulous. But the authors argue that adapting a more interventionist approach to data collection can help researchers avoid replicating historical and representational biases. A list of proposed integrations from archival science that stood out to me include:   Data consortia, which would set up institutional frameworks for ethical data collection to benefit both small and large firms.  Resources that ensure transparency, such as Datasheets for Datasets, which record not only the contents of data, but also the process of data collection.  A code of ethics for data collection with appropriate incentive measures to ensure compliance.The authors conclude with a macro-list of action items:   Congregate and develop data consortia  Establish professional organizations that work by member-ship to enforce adherence to ethical guidelines  Support community archives  Develop a subfield dedicated to the data collection and an-notation processMy Takeaways The idea of treating data collection carefully is not new to me; in economics research, generally an entire section of a paper details data collection methods and the overarching statistical properties of the data. I also have experience with archival research through a research project I completed in college. However, this paper has made me realize that just looking at superficial metadata like summary statistics is woefully inadequate. The process for data curation outlined here is so thorough and thoughtful, and I genuinely want to live in a world where data collection is treated with more reverence like explicated in this research. I’m particularly struck by this recommendation to the data science community: “form or integrate existing global/national organizations in instituting standardized codes of ethics/conduct and procedures to review violations.” So often, when I think about ML ethics, it’s hard for me to envision a sufficient incentive structure. I think the authors succeed in depicting the archival model, or something more like it, as a means of safeguarding ML models. ","categories": ["ai-ethics","research-review"],
        "tags": [],
        "url": "http://localhost:4000/ai-ethics/research-review/lessons-from-archives/"
      },{
        "title": "From What to How",
        "excerpt":"From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices A 2019 review of AI ethics tools and methods Publisher: Morley, J., Floridi, L., Kinsey, L. et al. From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices. Sci Eng Ethics 26, 2141–2168 (2020). https://doi.org/10.1007/s11948-019-00165-5 Summary The authors begin by asserting that code, and particularly code that facilitates machine learning, is both our greatest threat and our greatest promise. Citing a review of AI ethics principles, the argues note that ethically-aligned machine learning is typically defined as having the following properties: beneficence, non-maleficence, autonomy, justice, and explainability. This consensus on the definition of AI creates a foundation upon which technologists can “communicate expectations and evaluate deliverables”. However, very few of these guidelines set technical expectations. The authors note that the next step in defining ethical machine learning guidelines is to translate the what outlined in the principles to how. To bridge this gap, the authors define a typology that maps the foundational principles above to each stage of the machine learning algorithm design process. They identified 425 papers which ostensibly answer the question of “how to develop an ethical algorithmic system”. The typology developed is intended to show machine learning developers what ethical ML tools are already available. The authors find that not all principles have tools for each stage of the algorithm design process. Explainability, which is more easily implemented than the other principles, is overrepresented in the toolset. Unlike beneficence, non-maleficence, autonomy, and justice, explainability is not a moral principle, so tools that enable transparency like LIME and SHAP are insufficient alone. Instead of ensuring explainability at the back end, the authors propose that transparency is prioritized at the beginning of the design process. The authors also note that few of the tools provide ways to assess the impact of an ML algorithm on an individual in a dataset. The deployment column of the typology is blank, highlighting the need for “pro- ethically designed human-computer interaction (at an individual level) or networks of ML systems (at a group level)”. The authors assert that not enough research has been done in the field of translating predictions to decisions in the context of algorithmic systems. Finally, the tools in the typology are generally not usable, as they are either not easily applicable in practice or too complex for a wide set of users, as in the case of open source code libraries. ML researchers from all disciplines will need to accept that: (1) AI is built on assumptions; (2) human behaviour is complex; (3) algorithms can have unfair consequences; (4) algorithmic predictions can be hard to interpret (Vaughan &amp; Wallach, 2016); (5) trade-offs are usually inevitable; and (6) positive, ethical features are open to progressive increase. Ethical principles should not be applied once or not, but should be regularly re-applied or applied differently, depending on changing needs throughout the ML development process. My Takeaways I don’t think this article provided me with anything I didn’t already know. Through my (albeit limited) review of AI ethics principles, it’s evident that a wide gap exists between framework and technical practice. The guidelines I tend to see fall on one of two extremes: high-level principles that outline the values algorithmic systems should embed, or low-level technical implementations of those values (such as transparency as in SHAP, or fairness as in fairlearn) without explicating a context for prioritizing those results within organizational and/or project goals. I appreciated that the authors’ literature review and typology reflected this sentiment, and that they provided a thorough synthesis of the existing ML ethics conversation. I think one of my most important takeaways is at the end of the article, where the authors describe what topics the ML research community prioritize. They put the most important topic at the bottom of the list: the evaluation and creation of pro-ethical business models and incentive structures that balance the costs and rewards of investing in ethical AI across society. I firmly believe that none of the other topics can be on the table, not a commitment to reproducibility and openness, not evaluation of currently implemented tools, not the development of a common language, etc. until ML developers have organizational buy-in. Organizational buy-in only occurs when ethical priorities align with financial priorities, and in some cases the only way to facilitate that alignment may be a fundamental restructuring of financial priorities. The authors note that they eventually hope to create a searchable database with AI ethics tools and methods, and I think that would be particularly helpful to practitioners. The authors note an urgent need to progress research in the AI for Social Good field, and having a comprehensive database of tools (with a common/standardized language to discuss concepts) would be a good first step to making that research feasible. Notable Quotes Citations ","categories": ["ai-ethics","research-review"],
        "tags": [],
        "url": "http://localhost:4000/ai-ethics/research-review/review-of-tools/"
      },{
        "title": "Mitigating Dataset Harms Requires Stewardship",
        "excerpt":"Mitigating Dataset Harms Requires Stewardship: Lessons from 1000 Papers Advocating for a distributed approach to dataset harms mitigation Publisher: Kenny Peng, Arunesh Mathur, and Arvind Narayanan. “Mitigating Dataset Harms Requires Stewardship: Lessons from 1000 Papers.” 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks (2021). Summary The author’s approach is to evaluate the life cycles of three datasets that have recently faced ethical scrutiny: Labeled Faces in the Wild (LFW), MS-Celeb-1M, andDukeMTMC. These datasets are used for face/person recognition tasks. After analyzing nearly 1,000 papers that cite using one or more of these datasets, the authors made the following findings:   Dataset retraction has a limited effect on mitigating harms  Licenses can be ineffective at restraining production use of datasets  Social and technological change can change the ethical concerns associated with a dataset  Current dataset management and citation practices have shortcomings that means they are sometimes ineffective at ensuring ethical use.Dataset retractions work on limiting the use of datasets by both making it unavailable and setting normative expectations around use. However, in the case of the datasets the author studied, formal retraction did not make much of a difference because the dataset was also available in other, informal places. Another way others can learn about retracted datasets is through the citation of retracted papers, which the authors note is a problem in the machine learning field. Derivatives of datasets, which are used for purposes outside the intentions of the dataset creators, can also raise concerns. Derivatives can include updates with relabeled features, models trained on those datasets, and other post-processing modifications meant to improve performance on the original task. The authors also found that non-commercial dataset licenses, like those used by the datasets analyzed, are sometimes ignored in practice. The licenses either do not prohibit distribution or are no longer available. The authors note that ethical norms have shifted between when many popular datasets were first published and the current day. Most egregiously, in the case of ImageNet, researchers discovered misogynistic and racist slurs in image labels nearly a decade after the dataset’s release, prompting the dataset maintainers to remove those images. Finally, the authors find that none of the datasets they studied were centrally managed or have stable identifiers for tracking purposes. The authors put forward a number of recommendations for dataset creators to mitigate potential harms:   Make licenses and dataset documentation clear and easily accessible  Maintain active stewardship over datasets to address ethical concerns as they arise  Conferences should take an active role in encouraging responsible dataset use, advocating for dataset management and citation practices, and incentivizing responsible dataset creation through new publishing avenuesMy Takeaways My main takeaway is that I am not surprised the authors found such glaring issues with popular benchmarking datasets. They note at one point that “creating datasets is already an undervalued activity”, so why would dataset creators feel incentivized to be better stewards of their data? I recently wrote a blog post about the significant fairness issues associated with another non-image recognition benchmarking dataset (Boston Housing), and many of the same problems abounded (included unclear documentation, no licenses, shifting ethical norms, etc). That dataset was prolific in machine learning benchmarking, and often in applications that stray far from the original intended use of the data. I think ultimately this phenomenon is a consequence of the academic publish-or-perish cycle. This cycle has created an environment where cutting edge machine learning algorithms (or slight improvements on them) are needed to pass publisher muster, and simultaneously there are no official datasets allocated for machine learning testing. So popular datasets become the norm for benchmarking with little attention paid to the potential impacts of using a given algorithm on those data. I think the authors’ recommendations are good first steps, and chief among them should be restructuring the publishing incentive structure (as noted by their recommendations around conferences). If dataset evaluation research is rewarded, people will do it! I consider the second-most important recommendation to be the establishment of a centralized data management structure/review body. As the authors note, Institutional Review Boards may be insufficient for this new class of ethical concerns, and, not to be too cliche, modern problems require modern solutions. I think back to an article I read some time ago about dataset archives as a possible centralized data management option - it’s interesting that the authors did not review that suggestion in their analysis. Overall, I have been seeing more field surveys out of arXiv, and I think that’s good! More holistic reviews of datasets in particular will only further the development and adoption of new ethical behavior norms. ","categories": ["ai-ethics","research-review"],
        "tags": [],
        "url": "http://localhost:4000/ai-ethics/research-review/mitigating-dataset-harms/"
      }]
